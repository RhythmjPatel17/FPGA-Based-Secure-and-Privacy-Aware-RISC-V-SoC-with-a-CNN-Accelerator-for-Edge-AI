{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with Wide Residual Network (WRN)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, math, random\n",
    "import numpy as np, pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import py7zr\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract CIFAR-10 train/test archives\n",
    "archive_path_train = '../input/cifar-10/train.7z'\n",
    "archive_path_test = '../input/cifar-10/test.7z'\n",
    "extract_folder = '/kaggle/temp'\n",
    "with py7zr.SevenZipFile(archive_path_train, 'r') as archive:\n",
    "    archive.extractall(path=extract_folder)\n",
    "with py7zr.SevenZipFile(archive_path_test, 'r') as archive:\n",
    "    archive.extractall(path=extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dataset and CutOut\n",
    "class CIFAR10CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_df=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.ids = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, img_id)).convert('RGB')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        if self.labels_df is not None:\n",
    "            label = int(self.labels_df.loc[self.labels_df['id']==int(img_id.split('.')[0]),'label_idx'].values[0])\n",
    "            return img, label\n",
    "        else: return img, int(img_id.split('.')[0])\n",
    "\n",
    "class CutOut(object):\n",
    "    def __init__(self, n_holes, length): self.n_holes, self.length = n_holes, length\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h,w), np.float32)\n",
    "        for _ in range(self.n_holes):\n",
    "            y, x = random.randint(0,h), random.randint(0,w)\n",
    "            y1,y2 = np.clip(y-self.length//2,0,h), np.clip(y+self.length//2,0,h)\n",
    "            x1,x2 = np.clip(x-self.length//2,0,w), np.clip(x+self.length//2,0,w)\n",
    "            mask[y1:y2,x1:x2]=0.\n",
    "        mask = torch.from_numpy(mask).expand_as(img)\n",
    "        return img*mask"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Transformations and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(15, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
    "    CutOut(1,8),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
    "])\n",
    "train_csv = pd.read_csv('../input/cifar-10/trainLabels.csv')\n",
    "label_names = sorted(train_csv['label'].unique())\n",
    "label_to_index = {label: idx for idx,label in enumerate(label_names)}\n",
    "train_csv['label_idx'] = train_csv['label'].map(label_to_index)\n",
    "train_dataset = CIFAR10CustomDataset('/kaggle/temp/train', train_csv, train_transform)\n",
    "test_dataset = CIFAR10CustomDataset('/kaggle/temp/test', transform=test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Wide Residual Network\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels,out_channels,3,stride,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "        self.conv2 = nn.Conv2d(out_channels,out_channels,3,1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride!=1 or in_channels!=out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "        out += identity\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "class WRN(nn.Module):\n",
    "    def __init__(self, depth, wide, num_classes=10, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.in_channels = 16\n",
    "        n = (depth-4)//6\n",
    "        k = wide\n",
    "        temp = [16,16*k,32*k,64*k]\n",
    "        self.conv1 = nn.Conv2d(3,16,3,1,1,bias=False)\n",
    "        self.block1 = self.wide_block(Block,temp[1],n,1,dropout_rate)\n",
    "        self.block2 = self.wide_block(Block,temp[2],n,2,dropout_rate)\n",
    "        self.block3 = self.wide_block(Block,temp[3],n,2,dropout_rate)\n",
    "        self.bn = nn.BatchNorm2d(temp[3])\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.linear = nn.Linear(temp[3],num_classes)\n",
    "    def wide_block(self, block, outputs, num_blocks, stride, dropout_rate):\n",
    "        strides = [stride]+[1]*(num_blocks-1)\n",
    "        layers=[]\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels,outputs,s,dropout_rate))\n",
    "            self.in_channels = outputs\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.avg_pool2d(out,8)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "def create_wrn(depth=28,wide=10,num_classes=10,dropout_rate=0.3):\n",
    "    return WRN(depth,wide,num_classes,dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model, Loss, Optimizer\n",
    "model = create_wrn().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.1, epochs=80, steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3, anneal_strategy='cos', div_factor=25, final_div_factor=1e4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0,0,0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(f'Epoch {epoch} | Loss: {running_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Testing / submission\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, ids in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            predicted_names = [label_names[idx] for idx in predicted.cpu().numpy()]\n",
    "            result += list(zip(ids, predicted_names))\n",
    "    submission_df = pd.DataFrame(result, columns=['id','label'])\n",
    "    submission_df['id'] = submission_df['id'].astype(int)\n",
    "    submission_df = submission_df.sort_values('id')\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run training and testing\n",
    "num_epochs=80\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(epoch)\n",
    "test(model,test_loader)\n",
    "print('Predictions saved to submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3","language": "python","name": "python3"},
  "language_info": {"name": "python","version": "3.10"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

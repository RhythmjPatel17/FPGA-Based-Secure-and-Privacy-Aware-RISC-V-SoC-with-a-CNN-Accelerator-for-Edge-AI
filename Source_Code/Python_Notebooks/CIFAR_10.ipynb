{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with Wide Residual Network (WRN)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import py7zr\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Extract CIFAR-10 dataset\n",
    "archive_path_train = '../input/cifar-10/train.7z'\n",
    "archive_path_test = '../input/cifar-10/test.7z'\n",
    "extract_folder = '/kaggle/temp'\n",
    "\n",
    "with py7zr.SevenZipFile(archive_path_train, mode='r') as archive:\n",
    "    archive.extractall(path=extract_folder)\n",
    "\n",
    "with py7zr.SevenZipFile(archive_path_test, mode='r') as archive:\n",
    "    archive.extractall(path=extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader\n",
    "class CIFAR10CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_df=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.ids = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, img_id)).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.labels_df is not None:\n",
    "            label = int(self.labels_df.loc[self.labels_df['id'] == int(img_id.split('.')[0]), 'label_idx'].values[0])\n",
    "            return img, label\n",
    "        else:\n",
    "            return img, int(img_id.split('.')[0])\n",
    "\n",
    "class CutOut(object):\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        for n in range(self.n_holes):\n",
    "            y = random.randint(0, h)\n",
    "            x = random.randint(0, w)\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "            mask[y1:y2, x1:x2] = 0.\n",
    "        mask = torch.from_numpy(mask).expand_as(img)\n",
    "        img = img * mask\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n",
    "    CutOut(n_holes=1, length=8),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load labels\n",
    "train_csv = pd.read_csv('../input/cifar-10/trainLabels.csv')\n",
    "label_names = sorted(train_csv['label'].unique())\n",
    "label_to_index = {label: idx for idx, label in enumerate(label_names)}\n",
    "train_csv['label_idx'] = train_csv['label'].map(label_to_index)\n",
    "\n",
    "# Datasets and loaders\n",
    "train_dataset = CIFAR10CustomDataset(img_dir='/kaggle/temp/train', labels_df=train_csv, transform=train_transform)\n",
    "test_dataset = CIFAR10CustomDataset(img_dir='/kaggle/temp/test', transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. Wide Residual Network\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.0):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "        out += identity\n",
    "        out = self.relu2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class WRN(nn.Module):\n",
    "    def __init__(self, depth, wide, num_classes=10, dropout_rate=0.0):\n",
    "        super(WRN, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        n = (depth - 4) // 6\n",
    "        k = wide\n",
    "        temp = [16, 16*k, 32*k, 64*k]\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1, bias=False)\n",
    "        self.block1 = self.wide_block(Block, temp[1], n, stride=1, dropout_rate=dropout_rate)\n",
    "        self.block2 = self.wide_block(Block, temp[2], n, stride=2, dropout_rate=dropout_rate)\n",
    "        self.block3 = self.wide_block(Block, temp[3], n, stride=2, dropout_rate=dropout_rate)\n",
    "        self.bn = nn.BatchNorm2d(temp[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear = nn.Linear(temp[3], num_classes)\n",
    "\n",
    "    def wide_block(self, block, outputs, num_blocks, stride, dropout_rate=0.0):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, outputs, stride, dropout_rate))\n",
    "            self.in_channels = outputs\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def create_wrn(depth=28, wide=10, num_classes=10, dropout_rate=0.3):\n",
    "    return WRN(depth, wide, num_classes, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. Model, Loss, Optimizer\n",
    "model = create_wrn().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.1,\n",
    "    epochs=80,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5. Training Function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"Epoch {epoch} | Loss: {running_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 6. Testing / Submission Function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, ids in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_names = [label_names[idx] for idx in predicted.cpu().numpy()]\n",
    "            result += list(zip(ids, predicted_names))\n",
    "    submission_df = pd.DataFrame(result, columns=['id', 'label'])\n",
    "    submission_df['id'] = submission_df['id'].astype(int)\n",
    "    submission_df = submission_df.sort_values(by='id')\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 7. Run Training and Testing\n",
    "num_epochs = 80\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train(epoch)\n",
    "test(model, test_loader)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
